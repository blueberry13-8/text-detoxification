{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing. MUST BE MOVED TO src/data/..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           reference  \\\n0  If Alkar is flooding her with psychic waste, t...   \n1                          Now you're getting nasty.   \n2           Well, we could spare your life, for one.   \n3          Ah! Monkey, you've got to snap out of it.   \n4                   I've got orders to put her down.   \n\n                                         translation  similarity  lenght_diff  \\\n0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309   \n1                        you're becoming disgusting.    0.749687     0.071429   \n2                      well, we can spare your life.    0.919051     0.268293   \n3                       monkey, you have to wake up.    0.664333     0.309524   \n4                         I have orders to kill her.    0.726639     0.181818   \n\n    ref_tox   trn_tox  \n0  0.014195  0.981983  \n1  0.065473  0.999039  \n2  0.213313  0.985068  \n3  0.053362  0.994215  \n4  0.009402  0.999348  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchmetrics as torchmetrics\n",
    "\n",
    "extracted_dir = '../data/interim/'\n",
    "tsv_path = extracted_dir + 'filtered.tsv'\n",
    "tsv_file = pd.read_csv(tsv_path, sep='\\t', index_col=0)\n",
    "\n",
    "tsv_file.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "class DeToxicityDataset(Dataset):\n",
    "    def __init__(self, dataframe, to_remove_word_cnt=5, vocab = None, tox_diff=0.3):\n",
    "        self.df = dataframe\n",
    "        self._preprocess_sentences(to_remove_word_cnt, tox_diff)\n",
    "        assert len(self.references) == len(self.translations)\n",
    "        self.vocab = vocab or self._create_vocab()\n",
    "\n",
    "    def _preprocess_sentences(self, to_remove_word_cnt, tox_diff):\n",
    "        # Swap all ref with trn where toxicity level is greater in ref\n",
    "        to_swap = self.df['ref_tox'] < self.df['trn_tox']\n",
    "        self.df.loc[to_swap, ['reference', 'translation']] = self.df.loc[to_swap, ['translation', 'reference']].values\n",
    "        self.df.loc[to_swap, ['ref_tox', 'trn_tox']] = self.df.loc[to_swap, ['trn_tox', 'ref_tox']].values\n",
    "\n",
    "        # Delete all rows where difference between ref_tox and trn_tox is less than tox_diff\n",
    "        self.df = self.df[self.df['ref_tox'] - self.df['trn_tox'] >= tox_diff]\n",
    "\n",
    "        # Tokenize sentences\n",
    "        self.df['tokenized_reference'] = self.df['reference'].apply(lambda text: word_tokenize(text))\n",
    "        self.df['tokenized_translation'] = self.df['translation'].apply(lambda text: word_tokenize(text))\n",
    "\n",
    "        # Collect all words and count their occurrence in sentences\n",
    "        all_sent = self.df['tokenized_translation'].tolist() + self.df['tokenized_reference'].tolist()\n",
    "        all_words = [word for sent in all_sent for word in sent]\n",
    "        token_counts = Counter(all_words)\n",
    "\n",
    "        # Remove all words which occur less or equal than 'to_remove_word_cnt'\n",
    "        unique_words = set(all_words)\n",
    "        for word in token_counts:\n",
    "            if token_counts[word] <= to_remove_word_cnt:\n",
    "                unique_words.remove(word)\n",
    "\n",
    "        # Leave only approved words in tokenized sentences\n",
    "        self.df['tokenized_reference'] = self.df['tokenized_reference'].apply(lambda tokens: [word for word in tokens if word in unique_words])\n",
    "        self.df['tokenized_translation'] = self.df['tokenized_translation'].apply(lambda tokens: [word for word in tokens if word in unique_words])\n",
    "\n",
    "        # self.df = self.df[self.df['tokenized_reference'].apply(lambda x: len(x) <= max_sent_len)]\n",
    "        # self.df = self.df[self.df['tokenized_translation'].apply(lambda x: len(x) <= max_sent_len)]\n",
    "        # self.df['tokenized_reference'] = self.df['tokenized_reference'].apply(lambda tokens: [special_symbols[2]] + tokens + [special_symbols[3]])\n",
    "        # self.df['tokenized_translation'] = self.df['tokenized_translation'].apply(lambda tokens: [special_symbols[2]] + tokens + [special_symbols[3]])\n",
    "        self.references = self.df['tokenized_reference'].tolist()\n",
    "        self.translations = self.df['tokenized_translation'].tolist()\n",
    "\n",
    "    def _create_vocab(self):\n",
    "        # creates vocabulary that is used for encoding\n",
    "        # the sequence of tokens (splitted sentence)\n",
    "        vocab = build_vocab_from_iterator(self.references + self.translations, specials=special_symbols)\n",
    "        vocab.set_default_index(UNK_IDX)\n",
    "        return vocab\n",
    "\n",
    "    def _get_reference(self, index: int) -> list:\n",
    "        # retrieves sentence from dataset by index\n",
    "        sent = self.references[index]\n",
    "        return self.vocab(sent)\n",
    "\n",
    "    def _get_translation(self, index: int) -> list:\n",
    "        # retrieves translation from dataset by index\n",
    "        sent = self.translations[index]\n",
    "        return self.vocab(sent)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.references)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[list, list]:\n",
    "        return self._get_reference(index), self._get_translation(index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.2\n",
    "train_dataframe, val_dataframe = train_test_split(tsv_file, test_size=VALIDATION_RATIO, random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "train_dataset = DeToxicityDataset(train_dataframe)\n",
    "val_dataset = DeToxicityDataset(val_dataframe, vocab=train_dataset.vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['<bos>',\n 'everyone',\n 'wants',\n 'to',\n 'know',\n 'if',\n 'you',\n \"'re\",\n 'scared',\n '.',\n '<eos>']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.vocab.lookup_tokens(train_dataset[0][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "max_size = 50\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def collate_batch(batch: list):\n",
    "    references_batch, translations_batch = [], []\n",
    "    for _ref, _trn in batch:\n",
    "        _ref, _trn = _ref[:max_size], _trn[:max_size]\n",
    "        if len(_ref) < max_size:\n",
    "            _ref = [PAD_IDX] * (max_size - len(_ref)) + _ref\n",
    "        if len(_trn) < max_size:\n",
    "            _trn = [PAD_IDX] * (max_size - len(_trn)) + _trn\n",
    "        references_batch.append(torch.tensor(_ref))\n",
    "        translations_batch.append(torch.tensor(_trn))\n",
    "\n",
    "    return torch.stack(references_batch), torch.stack(translations_batch)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n"
     ]
    }
   ],
   "source": [
    "# just to check that all shapes are correct\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    inp, out = batch\n",
    "    print(inp.shape)\n",
    "    print(out.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DeToxicTranslator(nn.Module):\n",
    "    def __init__(self,  vocab_size, embedding_dim, lstm_dim, lstm_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_dim, num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.Linear(lstm_dim * 2, lstm_dim),\n",
    "            # nn.Dropout(0.25),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(lstm_dim, ),\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        print(x.shape)\n",
    "        x, hidden = self.lstm(x)\n",
    "        print(x.shape)\n",
    "        # x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "# from torchmetrics.text.bleu import BLEUScore\n",
    "# # ALSO, WE CAN USE NLTK FOR THAT\n",
    "# import nltk.translate.bleu_score as bleu\n",
    "# from nltk.translate.meteor_score import single_meteor_score\n",
    "#\n",
    "# # IT'S WORKING METRICS BTW"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "# from torchmetrics.text.bleu import BLEUScore\n",
    "\n",
    "def train_one_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    epoch_num=-1\n",
    "):\n",
    "    loop = tqdm(\n",
    "        enumerate(loader, 1),\n",
    "        total=len(loader),\n",
    "        desc=f\"Epoch {epoch}: train\",\n",
    "        leave=True,\n",
    "    )\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "    for i, batch in loop:\n",
    "        texts, labels = batch\n",
    "        texts = texts.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(texts).to(device)\n",
    "        # loss calculation\n",
    "        print(outputs.shape, labels.shape)\n",
    "        loss = loss_fn(outputs.to(device), labels.to(device))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # optimizer run\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        total += len(batch)\n",
    "        loop.set_postfix({\"loss\": train_loss/total})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "INPUT_DIM = len(train_dataset.vocab)\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_DIM = 50\n",
    "LSTM_LAYERS = 2\n",
    "\n",
    "model = DeToxicTranslator(INPUT_DIM, EMBEDDING_DIM, LSTM_DIM, LSTM_LAYERS).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: train:   0%|          | 0/3612 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50, 128])\n",
      "torch.Size([128, 50, 100])\n",
      "torch.Size([128, 50, 1])\n",
      "torch.Size([128, 50, 1]) torch.Size([128, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [128, 1], got [128, 50]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m----> 4\u001B[0m     \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# best = val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far=best)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[60], line 30\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, loader, optimizer, loss_fn, epoch_num)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# loss calculation\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs\u001B[38;5;241m.\u001B[39mshape, labels\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 30\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# backward pass\u001B[39;00m\n\u001B[0;32m     32\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m-> 1179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1180\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1181\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3052\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected target size [128, 1], got [128, 50]"
     ]
    }
   ],
   "source": [
    "best = -float('inf')\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch_num=epoch)\n",
    "    # best = val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far=best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}